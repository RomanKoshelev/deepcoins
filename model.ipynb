{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash model\n",
    "- https://papers.nips.cc/paper/4808-hamming-distance-metric-learning.pdf\n",
    "- https://arxiv.org/pdf/1702.00758.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# fix euclidean metric\n",
    "# - add batch normalization https://habrahabr.ru/company/wunderfund/blog/315476/\n",
    "# - add dropouts\n",
    "# - remove some maxpool layers\n",
    "# - Select from results by model.get_similarity(query_img, result_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from dataset import TripletDataset\n",
    "from augmentator import Augmentator\n",
    "from visualisation import show_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, image_shape, out_dims, metric):\n",
    "        assert metric=='hamming' or metric=='euclidean'\n",
    "        self.image_shape = image_shape\n",
    "        self.out_dims    = out_dims\n",
    "        self.metric      = metric\n",
    "        self._session    = None\n",
    "        self._graph      = None\n",
    "        \n",
    "    def _make_nn(self, images, out_dim, reuse):\n",
    "        def conv2d_maxpool(inputs, filters, name, reuse, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu):\n",
    "            l = tf.layers.conv2d(\n",
    "                inputs      = inputs,\n",
    "                filters     = filters,\n",
    "                kernel_size = kernel_size,\n",
    "                padding     = padding,\n",
    "                activation  = tf.nn.relu,\n",
    "                reuse       = reuse,\n",
    "                name        = name\n",
    "            )\n",
    "            l = tf.layers.max_pooling2d(l, pool_size=[2, 2], strides=2)\n",
    "            return l\n",
    "        l = images                                              # 128 x 128 x 1\n",
    "        l = conv2d_maxpool(l, 16,  reuse=reuse, name='conv1')   #  64 x 64  x 16\n",
    "        l = conv2d_maxpool(l, 32,  reuse=reuse, name='conv2')   #  32 x 32  x 32\n",
    "        l = conv2d_maxpool(l, 64,  reuse=reuse, name='conv3')   #  16 x 16  x 64\n",
    "        l = conv2d_maxpool(l, 128, reuse=reuse, name='conv4')   #   8 x 8   x 128\n",
    "        l = conv2d_maxpool(l, 256, reuse=reuse, name='conv5')   #   4 x 4   x 256 = 4096\n",
    "        l = tf.contrib.layers.flatten(l)\n",
    "        l = tf.layers.dense(l, units=2000, activation=tf.nn.relu, reuse=reuse, name='fc1')\n",
    "        l = tf.layers.dense(l, units=out_dim, reuse=reuse, name='logits')\n",
    "        \n",
    "        if self.metric == 'hamming':\n",
    "            l = tf.nn.tanh(l*100)\n",
    "        elif self.metric == 'euclidean':\n",
    "            l = tf.nn.tanh(l)\n",
    "        return l\n",
    "\n",
    "    def _make_loss(self, main, same, diff, margin):\n",
    "        def hamming_distance(a, b):\n",
    "            prod = tf.reduce_sum(tf.multiply(a, b), axis=1)\n",
    "            dist = (self.out_dims - prod)/2\n",
    "            return dist\n",
    "        def euclidean_distance(a, b):\n",
    "            dist = tf.sqrt(tf.reduce_sum((a-b)**2, axis=1))\n",
    "            return dist\n",
    "        \n",
    "        if self.metric == 'hamming':\n",
    "            dist = hamming_distance\n",
    "        elif self.metric == 'euclidean':\n",
    "            dist = euclidean_distance\n",
    "            \n",
    "        pos_dist = dist(main, same)\n",
    "        neg_dist = dist(main, diff)\n",
    "        loss = tf.nn.relu(pos_dist - neg_dist + margin)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "    def build(self):\n",
    "        tf.reset_default_graph()\n",
    "        self._graph = tf.Graph()\n",
    "        img_shape = self.image_shape\n",
    "        with self._graph.as_default(), tf.device('/gpu:0'):\n",
    "            # placeholders\n",
    "            self.img_main_pl = tf.placeholder(dtype=tf.float32, shape=[None,]+img_shape, name='main_img')\n",
    "            self.img_same_pl = tf.placeholder(dtype=tf.float32, shape=[None,]+img_shape, name='same_img')\n",
    "            self.img_diff_pl = tf.placeholder(dtype=tf.float32, shape=[None,]+img_shape, name='diff_img')\n",
    "            self.margin_pl   = tf.placeholder(dtype=tf.float32, name='margin')\n",
    "            self.b_pl        = tf.placeholder(dtype=tf.float32, name='b')\n",
    "            self.lr_pl       = tf.placeholder(dtype=tf.float32, name='lr')\n",
    "            # network\n",
    "            self.nn_main   = self._make_nn(self.img_main_pl, self.out_dims, reuse=False)\n",
    "            self.nn_same   = self._make_nn(self.img_same_pl, self.out_dims, reuse=True)\n",
    "            self.nn_diff   = self._make_nn(self.img_diff_pl, self.out_dims, reuse=True)\n",
    "            # operations\n",
    "            self.loss_op   = self._make_loss(self.nn_main, self.nn_same, self.nn_diff, self.margin_pl)\n",
    "            self.train_op  = tf.train.AdamOptimizer(self.lr_pl).minimize(self.loss_op)\n",
    "            self.init_op   = tf.global_variables_initializer()\n",
    "            \n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        self._session.run(self.init_op)\n",
    "        \n",
    "    def train(self, dataset, step_num, batch_size, margin, lr, aug, log_every=10, mean_win=100):\n",
    "        try:\n",
    "            losses = []\n",
    "            for step in range(step_num):\n",
    "                img_main, img_same, img_diff = dataset.get_next_batch(batch_size)\n",
    "                _, loss = self._session.run([self.train_op, self.loss_op], feed_dict={\n",
    "                    self.img_main_pl: aug(img_main),\n",
    "                    self.img_same_pl: aug(img_same),\n",
    "                    self.img_diff_pl: aug(img_diff),\n",
    "                    self.margin_pl:   margin,\n",
    "                    self.lr_pl:       lr,\n",
    "                })\n",
    "                losses.append(loss)\n",
    "                if step % log_every == log_every-1:\n",
    "                    show_losses(losses, step, step_num, mean_win)\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        show_losses(losses, step, step_num, mean_win)\n",
    "\n",
    "    def save(self, path):\n",
    "        with self._graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "        saver.save(self._session, path)\n",
    "        \n",
    "    def restore(self, path):\n",
    "        with self._graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "        saver.restore(self._session, path)        \n",
    "    \n",
    "    def run(self, imgs, batch_size=100, b=100):\n",
    "        outputs = np.zeros(shape=[len(imgs), self.out_dims])\n",
    "        bs = min(len(imgs), batch_size)\n",
    "        for i in range(len(imgs)//bs):\n",
    "            batch = imgs[i*bs:(i+1)*bs,:]\n",
    "            outputs[i*bs:(i+1)*bs,:] = self._session.run(self.nn_main, feed_dict = {\n",
    "                self.img_main_pl : batch,\n",
    "                self.b_pl : b,\n",
    "            })\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.72 s, sys: 56 ms, total: 2.77 s\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_size   = 100\n",
    "image_shape = [128,128,1]\n",
    "out_dims    = 7\n",
    "margin      = 1\n",
    "metric      = 'hamming' # euclidean / hamming\n",
    "\n",
    "dataset     = TripletDataset(image_shape)\n",
    "dataset.load(\"/netforge/datasets/private/roman/coins/images\", data_size)\n",
    "\n",
    "model = Model(image_shape, out_dims, metric)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'margin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'margin' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tr_aug = Augmentator(\n",
    "    cache_size = 10, \n",
    "    rate       = 1)\n",
    "\n",
    "model.train(\n",
    "    dataset    = dataset,\n",
    "    aug        = tr_aug.augment,\n",
    "    margin     = margin,\n",
    "    step_num   = 1000, \n",
    "    batch_size = 64, \n",
    "    lr         = 1e-5)\n",
    "model.save(\"data/hash_model/001/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "class DataBase:\n",
    "    def _get_embeds(self, images):\n",
    "        outputs = self.model.run(images)\n",
    "        embeds  = np.sign((outputs+1)/2)\n",
    "        return embeds\n",
    "        \n",
    "    def build(self, model, images, metric):\n",
    "        self.model   = model\n",
    "        self.images  = np.copy(images)\n",
    "        self.embeds  = self._get_embeds(images)\n",
    "        self._tree   = BallTree(self.embeds, metric=metric)\n",
    "        \n",
    "    def query(self, images, k=1):\n",
    "        embeds    = self._get_embeds(images)\n",
    "        dist, ind = self._tree.query(embeds, k)\n",
    "        return ind, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = DataBase()\n",
    "database.build(model, dataset.test_images[:1000], model.metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18]\n",
      " [99]\n",
      " [99]\n",
      " [99]\n",
      " [ 4]\n",
      " [18]\n",
      " [18]\n",
      " [18]\n",
      " [18]\n",
      " [ 9]]\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 6.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "te_aug  = Augmentator(rate=0)\n",
    "request = te_aug.augment(database.images[:10])\n",
    "\n",
    "ind, dist = database.query(request,1)\n",
    "\n",
    "print(ind)\n",
    "# print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from data/hash_model/001/\n"
     ]
    }
   ],
   "source": [
    "model.restore(\"data/hash_model/001/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
